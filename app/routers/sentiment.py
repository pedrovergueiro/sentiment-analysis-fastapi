"""
ü§ñ Rotas de An√°lise de Sentimentos

Aqui est√° o cora√ß√£o da minha API de IA!
Este arquivo cont√©m a l√≥gica para analisar sentimentos usando Machine Learning.

O que aprendi implementando isso:
- Como carregar modelos de IA uma √∫nica vez (otimiza√ß√£o)
- Como integrar HuggingFace Transformers com FastAPI
- Como tratar erros quando modelos n√£o est√£o dispon√≠veis
- Como estruturar respostas de APIs de IA
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from transformers import pipeline

# Criando o roteador para agrupar endpoints de IA
router = APIRouter(prefix="/sentiment", tags=["ü§ñ An√°lise de Sentimentos"])

# ========== CARREGAMENTO DO MODELO DE IA ==========
# IMPORTANTE: Carrego o modelo UMA VEZ quando a aplica√ß√£o inicia
# Isso evita recarregar o modelo a cada requisi√ß√£o (seria muito lento!)
try:
    print("ü§ñ Carregando modelo de IA...")
    # Pipeline do HuggingFace para an√°lise de sentimentos
    # Na primeira vez, vai baixar o modelo da internet (pode demorar)
    classifier = pipeline("sentiment-analysis")
    print("‚úÖ Modelo de IA carregado com sucesso!")
except Exception as e:
    print(f"‚ùå Erro ao carregar modelo: {e}")
    # Se n√£o conseguir carregar (sem internet, erro, etc.), define como None
    classifier = None

# ========== SCHEMAS DE DADOS ==========
class TextInput(BaseModel):
    """
    Schema para validar a entrada de texto.
    
    O Pydantic garante que sempre recebemos um texto v√°lido.
    """
    text: str
    
    class Config:
        # Exemplo para a documenta√ß√£o autom√°tica
        schema_extra = {
            "example": {
                "text": "Eu amo programar em Python! √â incr√≠vel!"
            }
        }

# ========== ENDPOINT PRINCIPAL DE IA ==========
@router.post("/")
def analyze_sentiment(data: TextInput):
    """
    ü§ñ Analisar sentimento de um texto usando IA
    
    Este √© o endpoint principal onde a m√°gica da IA acontece!
    
    **Como funciona:**
    1. Recebo um texto em linguagem natural
    2. Passo o texto para o modelo de IA (Transformer)
    3. O modelo retorna POSITIVE ou NEGATIVE + score de confian√ßa
    4. Retorno o resultado formatado
    
    **O que aprendi:**
    - Modelos de IA podem falhar (sem internet, erro de carregamento)
    - Sempre tratar erros graciosamente
    - Transformers retornam label + score
    - Score vai de 0 a 1 (confian√ßa da predi√ß√£o)
    """
    
    # Verificar se o modelo est√° dispon√≠vel
    if classifier is None:
        raise HTTPException(
            status_code=503, 
            detail="üö´ Modelo de IA n√£o est√° dispon√≠vel. Tente novamente mais tarde."
        )
    
    try:
        # AQUI ACONTECE A AN√ÅLISE DE IA! ü§ñ
        # O modelo processa o texto e retorna uma predi√ß√£o
        result = classifier(data.text)[0]
        
        # Formatando a resposta de forma clara
        return {
            "text": data.text,
            "label": result.get("label"),  # POSITIVE ou NEGATIVE
            "score": float(result.get("score", 0.0)),  # Confian√ßa (0-1)
            "interpretation": {
                "sentiment": "Positivo" if result.get("label") == "POSITIVE" else "Negativo",
                "confidence": f"{float(result.get('score', 0.0)) * 100:.1f}%"
            }
        }
        
    except Exception as e:
        # Se algo der errado durante a an√°lise
        raise HTTPException(
            status_code=500,
            detail=f"‚ùå Erro durante an√°lise: {str(e)}"
        )

@router.get("/health")
def check_model_health():
    """
    üè• Verificar se o modelo de IA est√° funcionando
    
    Endpoint √∫til para monitoramento e debugging.
    """
    if classifier is None:
        return {
            "status": "unhealthy",
            "model_loaded": False,
            "message": "Modelo n√£o est√° carregado"
        }
    
    try:
        # Teste r√°pido com texto simples
        test_result = classifier("test")[0]
        return {
            "status": "healthy",
            "model_loaded": True,
            "message": "Modelo funcionando corretamente",
            "test_prediction": test_result
        }
    except Exception as e:
        return {
            "status": "unhealthy", 
            "model_loaded": True,
            "message": f"Modelo carregado mas com erro: {str(e)}"
        }
